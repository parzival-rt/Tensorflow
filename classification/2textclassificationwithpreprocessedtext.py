# -*- coding: utf-8 -*-
"""2TextClassificationWithPreprocessedText.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L7H_ajxb51Oa6v-wZWJPVZounT_uZYSw

#**Text classification with preprocessed text: Movie reviews**
"""

import tensorflow as tf
from tensorflow import keras
!pip install -q tensorflow-datasets
import tensorflow_datasets as tfds
tfds.disable_progress_bar()
if tf.__version__ != '2.0.0':
  !pip install tensorflow==2.0.0
  print("** Tensorflow updated **")
print("Tensorflow version : ",tf.__version__)

"""#**Download the IMDB dataset**
It has already been preprocessed so that the reviews (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary.
"""

(trainData, testData), info = tfds.load(
    # Use the version pre-encoded with an ~8k vocabulary.
    'imdb_reviews/subwords8k',
    # Return the train/test datasets as a tuple
    split = (tfds.Split.TRAIN, tfds.Split.TEST),
    # Return (example, label) pairs from the dataset (instead of a dictionary).
    as_supervised = True,
    # Also return the `info` structure. 
    with_info=True)

"""#**Try the encoder**
The dataset info includes the text encoder (a tfds.features.text.SubwordTextEncoder).
"""

encoder = info.features['text'].encoder

print("Vocabulary size : {}".format(encoder.vocab_size))

"""This text encoder will reversibly encode any string:"""

sampleString = "Hello Tensorflow"
encodedString = encoder.encode(sampleString)
print("Encoded string : {}".format(encodedString))
originalString = encoder.decode(encodedString)
print("The original string : {}".format(originalString))

#assert originalString == sampleString

"""The encoder encodes the string by breaking it into subwords or characters if the word is not in its dictionary. So the more a string resembles the dataset, the shorter the encoded representation will be."""

for ts in encodedString:
  print("{} --> {}".format(ts, encoder.decode([ts])))

"""#**Explore the data**
* Let's take a moment to understand the format of the data. The dataset comes preprocessed: each example is an array of integers representing the words of the movie review.
* The text of reviews have been converted to integers, where each integer represents a specific word-piece in the dictionary.
* Each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.
"""

for trainExample, trainLabel in trainData.take(1):
  print("Encoding text : {]}",trainExample[:10].numpy())
  print("Label : ",trainLabel.numpy())

encoder.decode(trainExample)

"""#**Prepare the data for training**
The reviews are all different lengths, so use padded_batch to zero pad the sequences while batching:
"""

BUFFER_SIZE = 1000
trainBatches = (trainData.shuffle(BUFFER_SIZE).padded_batch(32, trainData.output_shapes))
testBatches = testData.padded_batch(32, trainData.output_shapes)

"""Each batch will have a shape of (batch_size, sequence_length) because the padding is dynamic each batch will have a different length:"""

for exampleBatch, labelBatch in trainBatches.take(2):
  print("Batch shape : ",exampleBatch.shape)
  print("Label : ",labelBatch.shape)

"""#**Build the model**
In this example, the input data consists of an array of word-indices. The labels to predict are either 0 or 1. Let's build a "Continuous bag of words" style model for this problem
"""

model = keras.Sequential([
  keras.layers.Embedding(encoder.vocab_size, 16),
  keras.layers.GlobalAveragePooling1D(),
  keras.layers.Dense(1, activation='sigmoid')])
model.summary()

"""##The layers are stacked sequentially to build the classifier:
1. The first layer is an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).
2. Next, a GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.
3. This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.
4. The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.

###Hidden units
If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patternsâ€”patterns that improve performance on training data but not on the test data. This is called overfitting, and we'll explore it later.

#**Loss function and optimizer**
"""

model.compile(optimizer='adam',
              loss='binary_crossentropy', # mean_squared_error
              metrics=['accuracy'])

"""#**Train the model**"""

history = model.fit(trainBatches,
                   epochs=10,
                   validation_data=testBatches,
                   validation_steps=30)

"""#**Evaluate the model**
And let's see how the model performs
"""

loss, accuracy = model.evaluate(testBatches)
print("Loss : ",loss)
print("Accuracy : ",accuracy)

"""#**Create a graph of accuracy and loss over time**"""

historyDict = history.history
historyDict.keys()

"""There are four entries. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy"""

import matplotlib.pyplot as plt

acc = historyDict['accuracy']
valAcc = historyDict['val_accuracy']
loss = historyDict['loss']
valLoss = historyDict['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, loss, 'bo', label='Training Loss') # 'bo' is for blue dot
plt.plot(epochs, valLoss, 'b', label='Validation Loss') # 'b' is for solid blue line
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend() # graph information
plt.show()

plt.clf() # clear figure

plt.plot(epochs, acc, 'bo', label='Training Acc')
plt.plot(epochs, valAcc, 'b', label='Validation Acc')
plt.title('Training and Validation Acc')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend(loc='lower right')
plt.show()

"""This is an example of **overfitting**. The model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations specific to the training data that do not generalize to test data.

#**The End**
"""